---
title: "ETF 5500 Assignment 2"
author: "Arindom Baruah (32779267)"
format: pdf

---


# Question 1

## Part A

Here, we have a data matrix $Y$ with a dimension of $n \times 3$ which has a mean of 0.

@eq-variance shows the equation of sample variance for a single variable $y$.

$$\text{Var}(y) =  \dfrac{1}{n-1} \sum_{j=1}^{n}(y_{i,j} - \bar{y_i})^2 $$ {#eq-variance}

Where, \
$y$ are the observations \
$\bar{y}$ is the mean of the observations \
$n$ is the number of observations 

While @eq-variance provides us with a singular value for a single variable, we would like to extend this idea for high dimensions in a matrix form. This matrix is called the variance-covariance matrix where every element in the diagonal provides us with the variance of the variable while every off-diagonal element provides us with the covariance of the variable with respect to another variable. This is given by @eq-varmat.

$$\text{Cov}(Y) =  \dfrac{1}{n-1}((Y-\bar{Y})'(Y-\bar{Y}) $$ {#eq-varmat}

Where, \

$Y$ is the data matrix, \
$\bar{Y}$ is the matrix containing the mean of each variable, \
$n$ is the number of observations in the data.

For a vector $Y$ that has been demeaned ($\bar{Y} = 0$), the sample variance-covariance matrix $S$ can be obtained through the following computation.

$$S = \text{Cov}(Y) =\dfrac{1}{n-1}(Y^TY)$$ {#eq-covmat}


To maintain matrix conformability, the dimension of $S$ will be a __$3 \times 3$__ matrix.

In order to define the covariance matric $S$ as defined by @eq-covmat, we require the following quanities:

1. A data matrix $Y$ with a mean of 0.
2. The total number of observations $n$ which is given by the number of rows in $Y$.


## Part B

An Eigen Value problem is linear in nature and defined by @eq-eigen.

$$Sw = \lambda w$$ {#eq-eigen}

Where,

$S = \text{Covariance Matrix}$ \
$w = \text{Eigen Vector}$ \
$\lambda = \text{Eigen Value}$ \

As the Eigen Vector is a column vector, hence, to satisfy matrix conformability in @eq-eigen, the dimension of $w$ must be a __$3 \times 1$__ vector.


## Part C

We are given the linear combination as stated through @eq-lineareq. 

$$X =  Y\beta$$ {#eq-lineareq}
We know that Y is a $n \times 1$ vector and $\beta$ is stated as a $3 \times 1$ vector. To obtain the Variance-Covariance matrix, we perform the following computations.

$$\text{Cov}(X) = \dfrac{1}{n-1}X'X$$

$$\implies \text{Cov}(X) =\sum_{i=1}^{n} \dfrac{(Y_i \beta)^2}{n-1}$$
$$\implies \text{Cov}(X) = \beta' \Bigl[ \sum_{i=1}^{n}\dfrac{(Y_i)^2}  {n-1}\Bigl] \beta$$

For a data matrix $Y_i$ with mean = 0, the term $\sum_{i=1}^{n}\dfrac{(Y_i)^2}  {n-1}$ is considered as the covariance and is denoted by $S$.

$$\boxed{\implies \text{Cov}(X) = \beta' S \beta}$$ {#eq-varcovcompute}


@eq-varcovcompute provides us with the final form of the variance-covariance matrix.

We know, the dimension of $\beta'$ is $1 \times 3$ and that of $\beta$ is $3 \times 1$. In order to maintain matrix conformability, the value dimension of the variance covariance matrix $\text{Var}(X)$ will be $1 \times 1$. This suggests that the __resultant matrix is a scalar value.__


# Exercise 2

The dimensions of the matrices of interest are as follows:

$w$ is a $3 \times 1$ matrix \
$Y$ is a $n \times 3$ matrix \
$S$ is a $3 \times 3$ matrix \
$v$ is an arbitrary column vector with a dimension $3 \times 1$

In order to assess matrix multiplication conformability, __the number of columns in the first matrix should be equal to the number of rows in the second matrix.__

## Part A

The matrix $Y$ is a general matrix with $n \times 3$ dimension. The only condition when the product $w'Y$ will be conformable is for a specific condition, $n = 3$. However, a data matrix with only 3 dimensions and only 3 observations is highly unlikely.

As a result, this product is expected to be __non-conformable.__

Under the special condition when $n = 3$, the dimension of $w'Y$ matrix will be $1 \times 3$.

## Part B

$ww'$ will be a conformable matrix with dimension $3 \times 3$.

## Part C

$wv'$ will be a __conformable matrix__ with a dimension $ 3 \times 3$.

## Part D

$S'Y$ matrix will be a __non-conformable matrix__ since $Y$ is an arbitrary column vector with no fixed dimension.


## Part E

$Yw + v'$ is __not a conformable matrix__ as the number of columns for matrix $Y$ and the number of rows for matrix $W$ are not identical. While adding the matrices, the dimensions of the two matrices must exactly match. This is however not the case here.


# Exercise 3

We are given with the equation $X = YC'$

Where $$
	C = \begin{bmatrix} 
	w' \\
	u'
	\end{bmatrix}
	\quad
	$$ and is a $2 \times 3$ dimensional matrix.

$w$ is the eigen vector of $S$ corresponding to the largest eigen value while $u$ is the eigen vector of $S$ corresponding to the second-largest eigen value.

Performing a matrix multiplication to obtain $X$ gives us a $n \times 2$ matrix where $n$ is the number of observations.


## Content of matrix X
The matrix multiplication performs a linear combination such that the data contained in matrix $Y$ is now projected along the eigen vectors relating to the largest and the second-largest eigen value. __In the context of principal component analysis (PCA), $X$ contains the data which is projected onto the top 2 principal components.__


## Derivation for sample covariance of X

We know, the variance-covariance matrix is calculated as follows:

$$\text{Cov}(X) = \dfrac{(X'X)}{n-1}$$
$$\implies \text{Cov}(X) =\sum_{i=1}^{n} \dfrac{(YC')^2}{n-1}$$

$$\implies \text{Cov}(X) = C' \Bigl[ \sum_{i=1}^{n}\dfrac{(Y_i)^2}  {n-1}\Bigl] C$$

As previously defined in @eq-covmat, we can use the matrix $S$ in the above equation as follows.

$$\boxed{\implies \text{Cov}(X) = C' S C}$$ {#eq-covx}

@eq-covx is the expression of the sample variance-covariance matrix $\text{Cov}(X)$ in terms of $C$ and $S$.


# Exercise 4

Vectors $w$ and $u$ are said to be orthogonal when they are at complete right angles to each other. This means that performing a matrix dot product, $w \cdot u$ will __yield a result of 0.__

In the context of PCA, vectors $w$ and $u$ (also called principal components) are __uncorrelated__ and capture unique signals in the data.

## Proof that Cov(X) is a diagonal matrix

From @eq-covx, we know that $\text{Cov}(X) = C' S C$.

Additionally, $w$ and $u$ are orthogonal.

Based on the spectral theorem, we can write the variance-covariance matrix $S$ as follows.

$$S = \Sigma\lambda_iv_iv_i'$$

Where $\lambda_i$ is an eigen value and $v_i$ is an eigen vector corresponding to the eigen value.

So, based on the above information, we can write @eq-covx as follows.


$$\text{Cov}(X) = \dfrac{1}{n-1}C^T (\lambda_w ww' + \lambda_u uu')C$$ 

$$\text{Cov}(X) = \dfrac{1}{n-1}\begin{bmatrix} 
	w' \\
	u'
	\end{bmatrix}
	\quad (\lambda_w ww' + \lambda_u uu')\begin{bmatrix} 
	w'
	u'
	\end{bmatrix}$$ {#eq-covmult}

All dot products between $w$ and $u$ reduce to 0 due to the orthogonality condition between the two vectors. 

Additionally, for normalised vectors, $vv' = 1$. As a result, @eq-covmult reduces to the following. 

$$\text{Cov}(X) = \dfrac{1}{n-1}(\lambda_w w + \lambda_u u) $$ {#eq-covvect}

Converting @eq-covvect to the matrix form, we can write it as follows.

$$\text{Cov}(X) = \dfrac{1}{n-1}\begin{bmatrix} 
	\lambda_w & 0\\
	0 & \lambda_u
	\end{bmatrix}
 $$ {#eq-covmatfinal}
 
 
The final form of the variance-covariance matrix is diagonal, as shown in @eq-covmatfinal . The diagonal elements contain the eigen values $\lambda_u$ and $\lambda_w$ corresponding to the eigen vectors $u$ and $w$ from the sample covariance matrix $S$.

This diagonal structure arises from the orthogonality condition, which ensures that all off-diagonal elements representing covariances are zero.

# Exercise 5


Here, we have observations give as follows.

$a = \dfrac{x_1}{\sqrt{\lambda_u}}$ and $b = \dfrac{x_2}{\sqrt{\lambda_u}}$

$Z$ is a datamatrix consisting of the vectors $a$ and $b$ as shown below. 

$$Z = \begin{bmatrix} 
	a & b
	\end{bmatrix}
 $$ 

Now, the sample variance-covariance matrix $\text{Cov (Z)}$ will be calculated as follows.

$$ \text{Cov}(Z) = \dfrac{1}{n-1}Z'Z$$
$$ \text{Cov}(Z) = \dfrac{1}{n-1}\begin{bmatrix} 
	a'a & a'b\\
	b'a & b'b
	\end{bmatrix}$$ {#eq-covmatz}
From @covmatfinal, we know that the variances of $x_1$ and $x_2$ are given by their eigen values. Using this result, we obtain the variances of the vectors $a$ and $b$ as follows.

$$\text{Var(a)} = \text{Var}\Bigl(\dfrac{x_1}{\sqrt{\lambda_w}}\Bigl)$$
Now, implementing the variance scaling rule $\text{Var(cX)} = c^2\text{Var(X)}$ and utilising results from @eq-covmatfinal to the above equation, we get the result as follows.

$$\text{Var(a)} = \dfrac{1}{\lambda_w}\text{Var}(x_1)$$

$$\implies \text{Var(a)} = \dfrac{\lambda_w}{\lambda_w} = 1$$

Similary, we obtain the value of $\text{Var}(b) = 1$.

For a demeaned vector upon normalisation, we know that,

$$\text{Var(a)} = \dfrac{1}{n-1}a'a$$
$$\implies \dfrac{1}{n-1}a'a = 1 $$
$$\implies a^Ta = n-1 $$ {#eq-var}
Similarly, $b^Tb = n - 1$

Next, we obtain the covariances between $a$ and $b$.

$$\text{Cov}(a,b) = \text{Cov}(\dfrac{x_1}{\lambda_w},\dfrac{x_2}{\lambda_u}) $$

$$\implies \text{Cov}(a,b) = \dfrac{1}{\sqrt{\lambda_w\lambda_u}}\text{Cov}({x_1},x_2) $$

However, we know that our vectors $x_1$ and $x_2$ are the columns in the matrix $X$, representing the two principal components PC1 and PC2. Hence, $x_1$ and $x_2$ are uncorrelated and will have a covariance $\text{Cov}(x_1,x_2) = 0$.

As a result, we obtain the following. 
$$\text{Cov}(a,b) = a'b = b'a = 0$$ {#eq-cov}

Replacing the relevant terms in @eq-covmatz by @eq-var and @eq-cov, we obtain the following result.


$$\text{Cov}(Z) = \dfrac{1}{n-1} \begin{bmatrix} 
	n-1 & 0\\
	0 & n-1
	\end{bmatrix} $$
	
	
$$\implies \text{Cov}(Z) = \begin{bmatrix} 
	1 & 0\\
	0 & 1
	\end{bmatrix} = \mathbb{I}$$ {#eq-covzidentity}

As observed in @eq-covzidentity, the sample variance-covariance matrix of $Z$ is an identity matrix.
