---
title: "ETF 5500 Assignment 2"
author: "Arindom Baruah (32779267)"
format: pdf

---


# Question 1

## Part A

Here, we have a data matrix $Y$ with a dimension of $n \times 3$ which has a mean of 0.

For a vector $Y$ which has been demeaned, the sample covariance matrix $S$ can be obtained through the following computation.

$$S = \dfrac{1}{n-1}(Y^TY)$$ {#eq-covmat}


To maintain matrix conformability, the dimension of $S$ will be a __$3 \times 3$__ matrix.

In order to define the covariance matric $S$ as defined by @eq-covmat, we require the following quanities:

1. A data matrix $Y$ with a mean of 0.
2. The total number of observations $n$ which is given by the number of rows in $Y$.


## Part B

An Eigen Value problem is linear in nature and defined by @eq-eigen.

$$SW = \lambda W$$ {#eq-eigen}

Where,

$S = \text{Covariance Matrix}$ \
$W = \text{Eigen Vector}$ \
$\lambda = \text{Eigen Value}$ \

As the Eigen Vector is a column vector, hence, to satisfy matrix conformability in @eq-eigen, the dimension of $W$ must be a __$3 \times 1$__ vector.


## Part C

We are given the linear combination as stated through @eq-lineareq. 

$$X = \beta Y$$ {#eq-lineareq}
We know that Y is a $n \times 1$ vector and $\beta$ is stated as a $3 \times 1$ vector. To obtain the Variance-Covariance matrix, we perform the following computations.

$$\text{Cov}(X) = \dfrac{1}{n-1}X^TX$$

$$\implies \text{Cov}(X) =\sum_{i=1}^{n} \dfrac{(Y_i \beta)^2}{n-1}$$
$$\implies \text{Cov}(X) = \beta^T \Bigl[ \sum_{i=1}^{n}\dfrac{(Y_i)^2}  {n-1}\Bigl] \beta$$

For a data matrix $Y_i$ with mean = 0, the term $\sum_{i=1}^{n}\dfrac{(Y_i)^2}  {n-1}$ is considered as the covariance and is denoted by $S$.

$$\boxed{\implies \text{Cov}(X) = \beta^T S \beta}$$ {#eq-varcovcompute}


@eq-varcovcompute provides us with the final form of the variance-covariance matrix.

We know, the dimension of $\beta^T$ is $1 \times 3$ and that of $beta$ is $3 \times 1$. In order to maintain matrix conformability, the value dimension of the variance covariance matrix $\text{Var}(X)$ will be $1 \times 1$. This suggests that the __resultant matrix is a scalar value.__


# Exercise 2

The dimensions of the matrices of interest are as follows:

$W$ is a $3 \times 1$ matrix \
$Y$ is a $n \times 1$ matrix \
$S$ is a $3 \times 3$ matrix \


## Part A

$W^TY$ matrix will be conformable __if the matrix $Y$ has a $n = 3$ observations.__

The dimension of this matrix will be $1 \times 3$ .

## Part B

$WW^T$ will be a conformable matrix with dimension $3 \times 3$.

## Part C

$WV^T$ will be a conformable matrix with dimension $3 \times 3$.

## Part D

$S^TY$ matrix will be conformable __if the matrix $Y$ has a $n = 3$ observations.__

The dimension of this new matrix will be $3 \times 3$.

## Part E

$YW + V^T$ is __not a conformable matrix__ as the number of columns for matrix $Y$ and the number of rows for matrix $W$ are not identical.


# Exercise 3

We are given with the equation $X = YC^T$

Where $$
	C = \begin{bmatrix} 
	W^T \\
	U^T
	\end{bmatrix}
	\quad
	$$ and is a $2 \times 3$ dimensional matrix.

$W$ is the eigen vector of $S$ corresponding to the largest eigen value while $U$ is the eigen vector of $S$ corresponding to the second-largest eigen value.

Performing a matrix multiplication to obtain $X$ gives us a $n \times 2$ matrix where $n$ is the number of observations.


## Content of matrix X
The matrix multiplication performs a linear combination such that the data contained in matrix $Y$ is now projected along the eigen vectors relating to the largest and the second-largest eigen value. __In the context of principal component analysis (PCA), $X$ contains the data which is projected onto the top 2 principal components.__


## Derivation for sample covariance of X

We know, the variance-covariance matrix is calculated as follows:

$$\text{Cov}(X) = \dfrac{(X^TX)}{n-1}$$
$$\implies \text{Cov}(X) =\sum_{i=1}^{n} \dfrac{(YC^T)^2}{n-1}$$

$$\implies \text{Cov}(X) = C^T \Bigl[ \sum_{i=1}^{n}\dfrac{(Y_i)^2}  {n-1}\Bigl] C$$

As previously defined in @eq-covmat, we can use the matrix $S$ in the above equation as follows.

$$\boxed{\implies \text{Cov}(X) = C^T S C}$$ {#eq-covx}

@eq-covx is the expression of the sample variance-covariance matrix $\text{Cov}(X)$ in terms of $C$ and $S$.


# Exercise 4

Vectors $W$ and $U$ are said to be orthogonal when they are at complete right angles to each other. This means that performing a matrix dot product, $W \cdot U$ will __yield a result of 0.__

In the context of PCA, vectors $W$ and $U$ (also called principal components) are __uncorrelated__ and capture unique signals in the data.

## Proof that Cov(X) is a diagonal matrix

From @eq-covx, we know that $\text{Cov}(X) = C^T S C$.

Additionally, $W$ and $U$ are orthogonal.

Based on the spectral theorem, we can write the variance-covariance matrix $S$ as follows.

$$S = \Sigma\lambda_iv_iv_i^T$$

Where $\lambda_i$ is an eigen value and $v_i$ is an eigen vector corresponding to the eigen value.

So, based on the above information, we can write @eq-covx as follows.


$$\text{Cov}(X) = C^T (\lambda_w WW^T + \lambda_u UU^T)C$$
All dot products between $W$ and $U$ were reduced to 0.
